{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "13f734529d5e4a30bcdc5d120fb62ac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_467165b240374fa1b37a32648fe6864b",
              "IPY_MODEL_cb32def1dbb14f75856906f0e0464dd1",
              "IPY_MODEL_41621b6ea2624ea5a7f033412cfcf176"
            ],
            "layout": "IPY_MODEL_eb6cf28095b5405f996be68ccc245f8e"
          }
        },
        "467165b240374fa1b37a32648fe6864b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_feade445a0664dfc8bd17add3c14e617",
            "placeholder": "​",
            "style": "IPY_MODEL_4c443614dfd14520beeb18d5133eaf14",
            "value": "Downloading builder script: 100%"
          }
        },
        "cb32def1dbb14f75856906f0e0464dd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5929e28f64f14dfbbecf6de7c3c5e6f7",
            "max": 8146,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3cdd2910fdda4a18a054ba7cdcde50e9",
            "value": 8146
          }
        },
        "41621b6ea2624ea5a7f033412cfcf176": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95db842d05fa4795845277edc787c090",
            "placeholder": "​",
            "style": "IPY_MODEL_5101f76d7f0c4cd48370e75811774197",
            "value": " 8.15k/8.15k [00:00&lt;00:00, 639kB/s]"
          }
        },
        "eb6cf28095b5405f996be68ccc245f8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "feade445a0664dfc8bd17add3c14e617": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c443614dfd14520beeb18d5133eaf14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5929e28f64f14dfbbecf6de7c3c5e6f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cdd2910fdda4a18a054ba7cdcde50e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "95db842d05fa4795845277edc787c090": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5101f76d7f0c4cd48370e75811774197": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cathieG/Prompt_Engineering/blob/main/zero_shot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6G5RUih7mNr",
        "outputId": "4db5af3b-aea1-4cda-be0a-2be9f1ef7d24",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.75.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting datasets>=2.0.0 (from evaluate)\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Collecting xxhash (from evaluate)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2.0.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.2)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-3.1.1 sacrebleu-2.5.1\n",
            "Collecting tree_sitter\n",
            "  Downloading tree_sitter-0.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.8 kB)\n",
            "Downloading tree_sitter-0.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (575 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m575.6/575.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tree_sitter\n",
            "Successfully installed tree_sitter-0.24.0\n"
          ]
        }
      ],
      "source": [
        "# Run the dependencies\n",
        "\n",
        "\n",
        "!pip3 install openai\n",
        "!pip install evaluate\n",
        "!pip install sacrebleu\n",
        "!pip install tree_sitter\n",
        "! git clone -q https://github.com/microsoft/CodeXGLUE.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Run this model in Python\n",
        "\n",
        "> pip install openai\n",
        "\"\"\"\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "import evaluate\n",
        "\n",
        "\n",
        "sacrebleu = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "# Create your PAT token by following instructions here:\n",
        "# https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens\n",
        "# grab the token from the token.txt file\n",
        "# with open(\"token.txt\", \"r\") as f:\n",
        "#     GITHUB_TOKEN = f.read().strip()\n",
        "\n",
        "# Set it as an environment variable\n",
        "# Insert your github token next to this line. As my github token is private, I obviously will not be showcasing it here.\n",
        "# os.environ[\"GITHUB_TOKEN\"] = GITHUB_TOKEN\n",
        "\n",
        "\n",
        "# To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings.\n",
        "client = OpenAI(\n",
        "    base_url=\"https://models.inference.ai.azure.com\",\n",
        "    api_key=os.environ[\"GITHUB_TOKEN\"],\n",
        ")\n",
        "\n",
        "# List of available models\n",
        "model_choices = [\"gpt-4o-mini\" , \"Codestral-2501\" , \"DeepSeek-R1\"]\n",
        "#DeepSeek is not allowed on gov owened machines"
      ],
      "metadata": {
        "id": "qVEi0_bb7sSj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238,
          "referenced_widgets": [
            "13f734529d5e4a30bcdc5d120fb62ac6",
            "467165b240374fa1b37a32648fe6864b",
            "cb32def1dbb14f75856906f0e0464dd1",
            "41621b6ea2624ea5a7f033412cfcf176",
            "eb6cf28095b5405f996be68ccc245f8e",
            "feade445a0664dfc8bd17add3c14e617",
            "4c443614dfd14520beeb18d5133eaf14",
            "5929e28f64f14dfbbecf6de7c3c5e6f7",
            "3cdd2910fdda4a18a054ba7cdcde50e9",
            "95db842d05fa4795845277edc787c090",
            "5101f76d7f0c4cd48370e75811774197"
          ]
        },
        "outputId": "8aaf5d62-da97-4a69-ec74-745d375f14b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13f734529d5e4a30bcdc5d120fb62ac6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cd /content/CodeXGLUE/Code-Code/code-to-code-trans/evaluator/CodeBLEU/parser && \\\n",
        "# bash build.sh java\n"
      ],
      "metadata": {
        "id": "H9GNZwUekmf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure you have the OpenAI client properly set up before running this\n",
        "# and that `client` and `model_choices` are defined.\n",
        "\n",
        "code_1 = [\n",
        "    '''\n",
        "    Prompt: \"Summarize what this function does in two sentences or less:\"\n",
        "\n",
        "    public Map<String, Integer> countWordFrequency(List<String> words) {\n",
        "    Map<String, Integer> freqMap = new HashMap<>();\n",
        "    for (String word : words) {\n",
        "        freqMap.put(word, freqMap.getOrDefault(word, 0) + 1);\n",
        "    }\n",
        "    return freqMap;\n",
        "}''',\n",
        "\n",
        "    '''\n",
        "    Prompt: \"Identify and fix the off-by one error. Please provide only the completed funcion:\"\n",
        "\n",
        "    def sum_range(start, end):\n",
        "    total = 0\n",
        "    for i in range(start, end):\n",
        "        total += i\n",
        "    return total''',\n",
        "\n",
        "    '''\n",
        "    Prompt: \"Classify the type of bug in the C++ function. Do nothing else besides this. This should be one sentence:\"\n",
        "\n",
        "    int* getArray(int size) {\n",
        "    int arr[size]; // Warning: local array\n",
        "    return arr; // Bug: returning pointer to local variable\n",
        "}''',\n",
        "\n",
        "    '''\n",
        "    Prompt: \"Complete the function using regex to validate basic email addresses. Don't explain your process, just give me the completed code:\"\n",
        "\n",
        "    def is_valid_email(email):\n",
        "    # TODO: Complete using regex\n",
        "    pass''',\n",
        "\n",
        "    '''\n",
        "    Prompt: \"Create a‘/greet/<username>‘endpoint that returns a JSON greeting. Don't explain your process. Just give me the completed code:\"\n",
        "\n",
        "    from flask import Flask, jsonify\n",
        "app = Flask(__name__)\n",
        "@app.route(’/greet/<username>’)\n",
        "def greet(username):\n",
        "    # TODO: Return a JSON greeting\n",
        "    pass''',\n",
        "\n",
        "    '''\n",
        "    Prompt: \"Write the schema for a review app with users,books,andreviews. Don't explain your process, just give me the end result:\"\n",
        "\n",
        "    -- TODO: Design schema with appropriate keys and constraints\n",
        "-- Tables: users(id, name), books(id, title), reviews(id, user_id, book_id, rating)''',\n",
        "\n",
        "    '''\n",
        "    Prompt: \"Identify any null dereference risks in the code and summarize them briefly in a sentence or two:\"\n",
        "\n",
        "    public int getLength(String s) {\n",
        "    return s.length(); // What if s is null?\n",
        "}''',\n",
        "\n",
        "    '''\n",
        "    Prompt: \"Improve the parser to support quoted fields. Don't explain your process, just give me the final code.\"\n",
        "\n",
        "    def parse_csv_line(line):\n",
        "    return line.split(’,’) # Incomplete: doesn’t handle quoted fields''',\n",
        "\n",
        "    '''\n",
        "    Prompt: \"Convert the data class to a  REST API using Ktor. Just give me the final code without explaining your process:\"\n",
        "\n",
        "    data class Product(val id: Int, val name: String, val price: Double)\n",
        "// TODO: Create GET and POST endpoints using Ktor''',\n",
        "\n",
        "    '''\n",
        "    Prompt: \"Summarize the function in two sentences or less:\"\n",
        "\n",
        "    def reverse_words(sentence):\n",
        "    return ’ ’.join(sentence.split()[::-1])''',\n",
        "\n",
        "    '''\n",
        "    Prompt: \"Write a prompt that could generate the code below. No need to explain your reasoning, you can just give me the prompt:\"\n",
        "\n",
        "    # This function checks if a number is prime\n",
        "def is_prime(n):\n",
        "    if n <= 1:\n",
        "        return False\n",
        "    for i in range (2 , int ( n **0.5) +1) :\n",
        "        if n % i == 0:\n",
        "            return False\n",
        "    return True''',\n",
        "\n",
        "    '''\n",
        "    Prompt: \"Fix the bug when the input is 0. Just give me the final code without explaining anything else:\"\n",
        "\n",
        "    def factorial(n):\n",
        "    result = 1\n",
        "    for i in range(1, n):\n",
        "        result *= i\n",
        "    return result''',\n",
        "\n",
        "    '''\n",
        "    Prompt: \"implement node deletion by value. Just give me the finished code, don't explain anything:\"\n",
        "\n",
        "    struct Node {\n",
        "    int data;\n",
        "    struct Node* next;\n",
        "};\n",
        "void deleteNode(struct Node** head, int key) {\n",
        "    // TODO: Implement node deletion\n",
        "}''',\n",
        "\n",
        "    '''\n",
        "    Prompt: \"Complete the recursive function for Fibonacci. Only give me the completed code. Don't explain anything:\"\n",
        "\n",
        "    def fibonacci(n):\n",
        "    # TODO: Base cases and recursive call\n",
        "    pass''',\n",
        "\n",
        "    '''\n",
        "    Prompt: \"Complete the class constructor. Don't explain your process, just give me the finished constructor.\"\n",
        "\n",
        "    class Person:\n",
        "    def __init__(self):\n",
        "        # TODO: Add name, age, and optional email\n",
        "        pass''',\n",
        "\n",
        "    '''\n",
        "    Prompt: \"Complete the binary search implementation and return only the final code without explanation:\n",
        "\n",
        "    public int binarySearch(int[] arr, int target) {\n",
        "    int left = 0, right = arr.length- 1;\n",
        "    while (left <= right) {\n",
        "        int mid = (left + right) / 2;\n",
        "        // TODO: Compare and adjust bounds\n",
        "    }\n",
        "    return -1;\n",
        "}''',\n",
        "\n",
        "    '''\n",
        "    Prompt: \"Resolve inconsistency between function name and logic. Don't explain what you did and why, just give me the corrected code:\"\n",
        "\n",
        "    // Supposed to return true if x is even\n",
        "bool isOdd(int x) {\n",
        "    return x % 2 == 0; // Logic contradicts function name\n",
        "}''',\n",
        "\n",
        "    '''\n",
        "    Prompt: \"Fix the bug in the function. Just give me the final code, and don't explain or write about anything else:\"\n",
        "\n",
        "    function isEven(n) {\n",
        "    return n % 2; // Returns 1 or 0, not true/false\n",
        "}''',\n",
        "\n",
        "    '''\n",
        "    Prompt: \"Take the sumamry/comment, decompose it, then rewrite it into more logical steps. Don't explain your process, just give me your final output:\"\n",
        "\n",
        "    // Function that validates an input, calculates square, and returns result\n",
        "int process(int x) {\n",
        "    if (x < 0) return-1;\n",
        "    return x * x;\n",
        "}''',\n",
        "\n",
        "    '''\n",
        "    Prompt: \"Finish the rest of the function. Just give me the final function, nothing else:\"\n",
        "\n",
        "    def calculate_average(scores):\n",
        "    total = 0\n",
        "    # TODO: Complete to return average\n",
        "    pass''',\n",
        "\n",
        "    '''\n",
        "\n",
        "    Prompt: \"Analyze the utility script, find any logical issues or design flaws, then refactor the code for better readability, correctness, and safety. Don't explain your process or reasoning. Just give me the refactored code with no comments:\"\n",
        "\n",
        "    # utils.py - Script to parse and summarize numeric CSV files\n",
        "import csv\n",
        "\n",
        "def read_csv(filepath):\n",
        "    with open(filepath, ’r’) as f:\n",
        "        return [row for row in csv.reader(f)]\n",
        "\n",
        "def summarize_column(data, index):\n",
        "    values = [float(row[index]) for row in data[1:]] # skip header\n",
        "    total = sum(values)\n",
        "    avg = total / len(values)\n",
        "    return total, avg\n",
        "\n",
        "def main():\n",
        "    filepath = ’data.csv’\n",
        "    data = read_csv(filepath)\n",
        "    total, avg = summarize_column(data, 1)\n",
        "    print(\"Total:\", total)\n",
        "    print(\"Average:\", avg)\n",
        "\n",
        "if __name__ == ’__main__’:\n",
        "    main()''',\n",
        "\n",
        "    '''\n",
        "\n",
        "    Prompt: \"Complete the following code, making sure you write in the missing code as well as cleaning up each line, removing punctuation, and making sure it counts word frequencies correctly. Don't explain your process. Just output the completed code:\"\n",
        "\n",
        "    # file_processor.py - Incomplete script for processing text files\n",
        "import string\n",
        "\n",
        "def load_file(filepath):\n",
        "    with open(filepath, ’r’) as f:\n",
        "        return f.readlines()\n",
        "\n",
        "def clean_line(line):\n",
        "    # TODO: Remove punctuation and make lowercase\n",
        "    pass\n",
        "\n",
        "def count_words(lines):\n",
        "    word_counts = {}\n",
        "    for line in lines:\n",
        "        clean = clean_line(line)\n",
        "        for word in clean.split():\n",
        "            word_counts[word] = word_counts.get(word, 0) + 1\n",
        "    return word_counts\n",
        "\n",
        "def main():\n",
        "    filepath = ’input.txt’\n",
        "    lines = load_file(filepath)\n",
        "    counts = count_words(lines)\n",
        "    for word, count in sorted(counts.items()):\n",
        "        print(f\"{word}: {count}\")\n",
        "\n",
        "if __name__ == ’__main__’:\n",
        "    main()'''\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "UHEkpuR9-XHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(len(code_test))"
      ],
      "metadata": {
        "id": "v_CTFZA39-GL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# zero_shot_prompts = [\"Summarize what this function does in two sentences or less:\",\n",
        "#                      \"Identify and fix the off-by one error. Please provide only the completed funcion\",\n",
        "#                      \"Classify the type of bug in the C++ function. Do nothing else besides this. This should be one sentence\",\n",
        "#                      \"Complete the function using regex to validate basic email addresses. Don't explain your process, just give me the completed code.\",\n",
        "#                      \"Create a‘/greet/<username>‘endpoint that returns a JSON greeting. Don't explain your process. Just give me the completed code.\",\n",
        "#                      \"Write the schema for a review app with users,books,andreviews. Don't explain your process, just give me the end result.\",\n",
        "#                      \"Identify any null dereference risks in the code and summarize them briefly in a sentence or two.\",\n",
        "#                      \"Improve the parser to support quoted fields. Don't explain your process, just give me the final code.\",\n",
        "#                      \"Convert the data class to a  REST API using Ktor. Just give me the final code without explaining your process.\",\n",
        "#                      \"Summarize the function in two sentences or less:\",\n",
        "#                      \"Write a prompt that could generate the code below. No need to explain your reasoning, you can just give me the prompt.\",\n",
        "#                      \"Fix the bug when the input is 0. Just give me the final code without explaining anything else.\",\n",
        "#                      \"implement node deletion by value. Just give me the finished code, don't explain anything.\",\n",
        "#                      \"Complete the recursive function for Fibonacci. Only give me the completed code. Don't explain anything.\",\n",
        "#                      \"Complete the class constructor. Don't explain your process, just give me the finished constructor.\",\n",
        "#                      \"Complete the binary search implementation and return only the final code without explanation.\",\n",
        "#                      \"Resolve inconsistency between function name and logic. Don't explain what you did and why, just give me the corrected code.\",\n",
        "#                      \"Fix the bug in the function. Just give me the final code, and don't explain or write about anything else.\",\n",
        "#                      \"Take the sumamry/comment, decompose it, then rewrite it into more logical steps. Don't explain your process, just give me your final output.\",\n",
        "#                      \"Finish the rest of the function. Just give me the final function, nothing else:\",\n",
        "#                      \"Analyze the utility script, find any logical issues or design flaws, then refactor the code for better readability, correctness, and safety. Don't explain your process or reasoning. Just give me the refactored code with no comments.\",\n",
        "#                      \"Complete the following code, making sure you write in the missing code as well as cleaning up each line, removing punctuation, and making sure it counts word frequencies correctly. Don't explain your process. Just output the completed code.\"]\n"
      ],
      "metadata": {
        "id": "qKUOvgPo_Zhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function that produces the output, then put that output into two seperate lists\n",
        "\n",
        "def prompt_model_zero(code):\n",
        "  Claude = []\n",
        "  GPT = []\n",
        "\n",
        "  i = 0\n",
        "\n",
        "  for code_snippets in code:\n",
        "\n",
        "    print(\"This is the code_snippet: \")\n",
        "    print(code_snippets)\n",
        "\n",
        "    response_Claude = client.chat.completions.create(\n",
        "        model=model_choices[1],\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\":code_snippets}\n",
        "        ],\n",
        "        max_tokens=1024,\n",
        "        temperature=0.3,\n",
        "    )\n",
        "\n",
        "    response_GPT = client.chat.completions.create(\n",
        "        model=model_choices[0],\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": code_snippets}\n",
        "        ],\n",
        "        max_tokens=1024,\n",
        "        temperature=0.3,\n",
        "    )\n",
        "\n",
        "    output_Claude = response_Claude.choices[0].message.content\n",
        "    Claude.append({\n",
        "        \"index\": i,\n",
        "        \"prompt\": zero_shot_prompts[i],\n",
        "        \"code_snippet\": code_snippets,\n",
        "        \"response\": output_Claude\n",
        "    })\n",
        "\n",
        "    output_GPT = response_GPT.choices[0].message.content\n",
        "    GPT.append({\n",
        "        \"index\": i,\n",
        "        \"prompt\": zero_shot_prompts[i],\n",
        "        \"code_snippet\": code_snippets,\n",
        "        \"response\": output_GPT\n",
        "    })\n",
        "\n",
        "    i += 1\n",
        "    print(f\"Response for Claude, Code Snippet {i}:\\n{response_Claude.choices[0].message.content}\\n{'-'*60}\\n\")\n",
        "    print()\n",
        "    print(f\"Response for GPT, Code Snippet {i}:\\n{response_GPT.choices[0].message.content}\\n{'-'*60}\\n\")\n",
        "    print()\n",
        "\n",
        "  return Claude, GPT"
      ],
      "metadata": {
        "id": "Abg--b6YTtat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the Code\n",
        "\n",
        "pred, ref = prompt_model_zero(code_1)\n",
        "\n",
        "# print(pred)\n",
        "# print(ref)\n",
        "# print(len(pred))\n",
        "# print(len(ref))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "id": "45kTtJftVhnv",
        "outputId": "955d8749-25dd-4ec4-a8d2-74697620c330",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the code_snippet: \n",
            "\n",
            "    Prompt: \"Improve the parser to support quoted fields. Don't explain your process, just give me the final code.\"\n",
            "\n",
            "    def parse_csv_line(line):\n",
            "    return line.split(’,’) # Incomplete: doesn’t handle quoted fields\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-d58e329eb638>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_model_zero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_shot_prompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-0c8cc989c858>\u001b[0m in \u001b[0;36mprompt_model_zero\u001b[0;34m(code, prompts)\u001b[0m\n\u001b[1;32m     19\u001b[0m     )\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     response_GPT = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_choices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    927\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    928\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1274\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m         )\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    950\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Put them into predictions and references\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "for i in range(22):\n",
        "  predictions.append(pred[i]['response'])\n",
        "  references.append(ref[i]['response'])"
      ],
      "metadata": {
        "id": "zCL4wzoj9BbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the evaluation scores for each output\n",
        "\n",
        "def Calc_Metrics_Zero(prediction, reference):\n",
        "  scores = []\n",
        "  selected_indices = {0, 2, 6, 9, 10, 18}\n",
        "  for i, (pred, ref) in enumerate(zip(predictions, references)):\n",
        "    print(i)\n",
        "    print(pred)\n",
        "    print(ref)\n",
        "    print()\n",
        "    if i in selected_indices:\n",
        "\n",
        "      result = sacrebleu.compute(predictions=[pred], references=[[ref]])\n",
        "      print(\"Bleu-4 Score: \"+ str(result['score']))\n",
        "      print()\n",
        "      scores.append(\"Bleu-4 Score: \" + str(round(result[\"score\"], 5)))\n",
        "\n",
        "    else:\n",
        "      # print(\"Exact Match: \"+ str(pred == ref))\n",
        "      # print()\n",
        "      scores.append(\"Match: \" +str(pred == ref))\n",
        "\n",
        "  return scores"
      ],
      "metadata": {
        "id": "rNSgAYb8LuX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "New_Scores = Calc_Metrics_Zero(predictions, references)\n",
        "# print(New_Scores)"
      ],
      "metadata": {
        "id": "vsdS2VKXLx1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03f746f4-90a5-4b6d-bd86-42ff3ef0ff22",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "This function takes a list of words and returns a map where the keys are the words from the list and the values are the frequencies of those words. It iterates through the list, updating the count for each word in the map.\n",
            "The function `countWordFrequency` takes a list of words as input and returns a map that counts the frequency of each word in the list. It uses a HashMap to store the words as keys and their corresponding counts as values, updating the count for each word as it iterates through the list.\n",
            "\n",
            "Bleu-4 Score: 30.65076310801197\n",
            "\n",
            "1\n",
            "```python\n",
            "def sum_range(start, end):\n",
            "    total = 0\n",
            "    for i in range(start, end + 1):\n",
            "        total += i\n",
            "    return total\n",
            "```\n",
            "```python\n",
            "def sum_range(start, end):\n",
            "    total = 0\n",
            "    for i in range(start, end + 1):\n",
            "        total += i\n",
            "    return total\n",
            "```\n",
            "\n",
            "2\n",
            "Returning pointer to local variable\n",
            "The bug is returning a pointer to a local variable, which leads to undefined behavior.\n",
            "\n",
            "Bleu-4 Score: 2.7409299216719143\n",
            "\n",
            "3\n",
            "```python\n",
            "import re\n",
            "\n",
            "def is_valid_email(email):\n",
            "    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
            "    return bool(re.match(pattern, email))\n",
            "```\n",
            "```python\n",
            "import re\n",
            "\n",
            "def is_valid_email(email):\n",
            "    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
            "    return re.match(pattern, email) is not None\n",
            "```\n",
            "\n",
            "4\n",
            "```python\n",
            "from flask import Flask, jsonify\n",
            "\n",
            "app = Flask(__name__)\n",
            "\n",
            "@app.route('/greet/<username>')\n",
            "def greet(username):\n",
            "    return jsonify(message=f\"Hello, {username}!\")\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    app.run(debug=True)\n",
            "```\n",
            "```python\n",
            "from flask import Flask, jsonify\n",
            "\n",
            "app = Flask(__name__)\n",
            "\n",
            "@app.route('/greet/<username>')\n",
            "def greet(username):\n",
            "    return jsonify(greeting=f\"Hello, {username}!\")\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    app.run(debug=True)\n",
            "```\n",
            "\n",
            "5\n",
            "```sql\n",
            "CREATE TABLE users (\n",
            "    id SERIAL PRIMARY KEY,\n",
            "    name VARCHAR(255) NOT NULL\n",
            ");\n",
            "\n",
            "CREATE TABLE books (\n",
            "    id SERIAL PRIMARY KEY,\n",
            "    title VARCHAR(255) NOT NULL\n",
            ");\n",
            "\n",
            "CREATE TABLE reviews (\n",
            "    id SERIAL PRIMARY KEY,\n",
            "    user_id INT NOT NULL,\n",
            "    book_id INT NOT NULL,\n",
            "    rating INT CHECK (rating >= 1 AND rating <= 5),\n",
            "    FOREIGN KEY (user_id) REFERENCES users(id),\n",
            "    FOREIGN KEY (book_id) REFERENCES books(id)\n",
            ");\n",
            "```\n",
            "```sql\n",
            "CREATE TABLE users (\n",
            "    id SERIAL PRIMARY KEY,\n",
            "    name VARCHAR(100) NOT NULL\n",
            ");\n",
            "\n",
            "CREATE TABLE books (\n",
            "    id SERIAL PRIMARY KEY,\n",
            "    title VARCHAR(255) NOT NULL\n",
            ");\n",
            "\n",
            "CREATE TABLE reviews (\n",
            "    id SERIAL PRIMARY KEY,\n",
            "    user_id INT NOT NULL,\n",
            "    book_id INT NOT NULL,\n",
            "    rating INT CHECK (rating >= 1 AND rating <= 5),\n",
            "    FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE,\n",
            "    FOREIGN KEY (book_id) REFERENCES books(id) ON DELETE CASCADE\n",
            ");\n",
            "```\n",
            "\n",
            "6\n",
            "The code contains a null dereference risk. If the input string `s` is `null`, calling `s.length()` will result in a `NullPointerException` because the `length()` method cannot be invoked on a `null` object. To mitigate this risk, you should add a null check before calling `s.length()`.\n",
            "The code contains a null dereference risk because if the input string `s` is `null`, calling `s.length()` will result in a `NullPointerException`. To mitigate this risk, a null check should be added before attempting to access the length of the string.\n",
            "\n",
            "Bleu-4 Score: 51.54408784027284\n",
            "\n",
            "7\n",
            "```python\n",
            "def parse_csv_line(line):\n",
            "    fields = []\n",
            "    field = []\n",
            "    in_quotes = False\n",
            "    escape_next = False\n",
            "\n",
            "    for char in line:\n",
            "        if escape_next:\n",
            "            field.append(char)\n",
            "            escape_next = False\n",
            "        elif char == '\\\\':\n",
            "            escape_next = True\n",
            "        elif char == '\"':\n",
            "            in_quotes = not in_quotes\n",
            "        elif char == ',' and not in_quotes:\n",
            "            fields.append(''.join(field))\n",
            "            field = []\n",
            "        else:\n",
            "            field.append(char)\n",
            "\n",
            "    fields.append(''.join(field))\n",
            "    return fields\n",
            "```\n",
            "```python\n",
            "import csv\n",
            "\n",
            "def parse_csv_line(line):\n",
            "    return next(csv.reader([line]))\n",
            "```\n",
            "\n",
            "8\n",
            "```kotlin\n",
            "import io.ktor.application.*\n",
            "import io.ktor.features.ContentNegotiation\n",
            "import io.ktor.http.*\n",
            "import io.ktor.request.*\n",
            "import io.ktor.response.*\n",
            "import io.ktor.routing.*\n",
            "import io.ktor.serialization.*\n",
            "import io.ktor.server.engine.*\n",
            "import io.ktor.server.netty.*\n",
            "\n",
            "data class Product(val id: Int, val name: String, val price: Double)\n",
            "\n",
            "fun main() {\n",
            "    embeddedServer(Netty, port = 8080) {\n",
            "        install(ContentNegotiation) {\n",
            "            json()\n",
            "        }\n",
            "        routing {\n",
            "            val products = mutableListOf<Product>()\n",
            "\n",
            "            get(\"/products\") {\n",
            "                call.respond(products)\n",
            "            }\n",
            "\n",
            "            post(\"/products\") {\n",
            "                val product = call.receive<Product>()\n",
            "                products.add(product)\n",
            "                call.respondText(\"Product stored correctly\", status = HttpStatusCode.Created)\n",
            "            }\n",
            "        }\n",
            "    }.start(wait = true)\n",
            "}\n",
            "```\n",
            "```kotlin\n",
            "import io.ktor.application.*\n",
            "import io.ktor.features.ContentNegotiation\n",
            "import io.ktor.http.HttpStatusCode\n",
            "import io.ktor.jackson.jackson\n",
            "import io.ktor.request.receive\n",
            "import io.ktor.response.respond\n",
            "import io.ktor.routing.*\n",
            "import io.ktor.server.engine.embeddedServer\n",
            "import io.ktor.server.netty.Netty\n",
            "import io.ktor.serialization.kotlinx.json.json\n",
            "import kotlinx.coroutines.sync.Mutex\n",
            "import kotlinx.coroutines.sync.withLock\n",
            "\n",
            "data class Product(val id: Int, val name: String, val price: Double)\n",
            "\n",
            "val products = mutableListOf<Product>()\n",
            "val mutex = Mutex()\n",
            "\n",
            "fun main() {\n",
            "    embeddedServer(Netty, port = 8080) {\n",
            "        install(ContentNegotiation) {\n",
            "            jackson {}\n",
            "        }\n",
            "        routing {\n",
            "            route(\"/products\") {\n",
            "                get {\n",
            "                    mutex.withLock {\n",
            "                        call.respond(products)\n",
            "                    }\n",
            "                }\n",
            "                post {\n",
            "                    val product = call.receive<Product>()\n",
            "                    mutex.withLock {\n",
            "                        products.add(product)\n",
            "                        call.respond(HttpStatusCode.Created, product)\n",
            "                    }\n",
            "                }\n",
            "            }\n",
            "        }\n",
            "    }.start(wait = true)\n",
            "}\n",
            "```\n",
            "\n",
            "9\n",
            "The `reverse_words` function takes a sentence as input and returns the sentence with the order of words reversed. It splits the sentence into words, reverses the list of words, and then joins them back into a single string.\n",
            "The `reverse_words` function takes a string `sentence`, splits it into individual words, reverses the order of those words, and then joins them back together into a single string with spaces in between. The result is the original sentence with the word order reversed.\n",
            "\n",
            "Bleu-4 Score: 39.66630260174928\n",
            "\n",
            "10\n",
            "Write a Python function named `is_prime` that takes an integer `n` as input and returns `True` if `n` is a prime number, and `False` otherwise.\n",
            "Write a Python function named is_prime that takes an integer n and is intended to check if the number \n",
            "is prime. Leave the function body unimplemented with a pass statement, and include a comment above \n",
            "the function explaining its purpose.\n",
            "\n",
            "Bleu-4 Score: 18.145592503801026\n",
            "\n",
            "11\n",
            "```python\n",
            "def factorial(n):\n",
            "    if n == 0:\n",
            "        return 1\n",
            "    result = 1\n",
            "    for i in range(1, n + 1):\n",
            "        result *= i\n",
            "    return result\n",
            "```\n",
            "```python\n",
            "def factorial(n):\n",
            "    if n == 0:\n",
            "        return 1\n",
            "    result = 1\n",
            "    for i in range(1, n + 1):\n",
            "        result *= i\n",
            "    return result\n",
            "```\n",
            "\n",
            "12\n",
            "```c\n",
            "struct Node {\n",
            "    int data;\n",
            "    struct Node* next;\n",
            "};\n",
            "\n",
            "void deleteNode(struct Node** head, int key) {\n",
            "    struct Node* temp = *head;\n",
            "    struct Node* prev = NULL;\n",
            "\n",
            "    // If head node itself holds the key to be deleted\n",
            "    if (temp != NULL && temp->data == key) {\n",
            "        *head = temp->next; // Changed head\n",
            "        free(temp);         // Free old head\n",
            "        return;\n",
            "    }\n",
            "\n",
            "    // Search for the key to be deleted, keep track of the\n",
            "    // previous node as we need to change 'prev->next'\n",
            "    while (temp != NULL && temp->data != key) {\n",
            "        prev = temp;\n",
            "        temp = temp->next;\n",
            "    }\n",
            "\n",
            "    // If key was not present in linked list\n",
            "    if (temp == NULL) return;\n",
            "\n",
            "    // Unlink the node from linked list\n",
            "    prev->next = temp->next;\n",
            "\n",
            "    free(temp); // Free memory\n",
            "}\n",
            "```\n",
            "```c\n",
            "#include <stdlib.h>\n",
            "\n",
            "struct Node {\n",
            "    int data;\n",
            "    struct Node* next;\n",
            "};\n",
            "\n",
            "void deleteNode(struct Node** head, int key) {\n",
            "    struct Node* temp = *head;\n",
            "    struct Node* prev = NULL;\n",
            "\n",
            "    // If the head node itself holds the key to be deleted\n",
            "    if (temp != NULL && temp->data == key) {\n",
            "        *head = temp->next; // Changed head\n",
            "        free(temp);         // Free old head\n",
            "        return;\n",
            "    }\n",
            "\n",
            "    // Search for the key to be deleted, keep track of the previous node\n",
            "    while (temp != NULL && temp->data != key) {\n",
            "        prev = temp;\n",
            "        temp = temp->next;\n",
            "    }\n",
            "\n",
            "    // If the key was not present in the linked list\n",
            "    if (temp == NULL) return;\n",
            "\n",
            "    // Unlink the node from linked list\n",
            "    prev->next = temp->next;\n",
            "\n",
            "    free(temp); // Free memory\n",
            "}\n",
            "```\n",
            "\n",
            "13\n",
            "```python\n",
            "def fibonacci(n):\n",
            "    if n <= 0:\n",
            "        return 0\n",
            "    elif n == 1:\n",
            "        return 1\n",
            "    else:\n",
            "        return fibonacci(n-1) + fibonacci(n-2)\n",
            "```\n",
            "```python\n",
            "def fibonacci(n):\n",
            "    if n <= 0:\n",
            "        return 0\n",
            "    elif n == 1:\n",
            "        return 1\n",
            "    else:\n",
            "        return fibonacci(n - 1) + fibonacci(n - 2)\n",
            "```\n",
            "\n",
            "14\n",
            "```python\n",
            "class Person:\n",
            "    def __init__(self, name, age, email=None):\n",
            "        self.name = name\n",
            "        self.age = age\n",
            "        self.email = email\n",
            "```\n",
            "```python\n",
            "class Person:\n",
            "    def __init__(self, name, age, email=None):\n",
            "        self.name = name\n",
            "        self.age = age\n",
            "        self.email = email\n",
            "```\n",
            "\n",
            "15\n",
            "```java\n",
            "public int binarySearch(int[] arr, int target) {\n",
            "    int left = 0, right = arr.length - 1;\n",
            "    while (left <= right) {\n",
            "        int mid = left + (right - left) / 2;\n",
            "        if (arr[mid] == target) {\n",
            "            return mid;\n",
            "        } else if (arr[mid] < target) {\n",
            "            left = mid + 1;\n",
            "        } else {\n",
            "            right = mid - 1;\n",
            "        }\n",
            "    }\n",
            "    return -1;\n",
            "}\n",
            "```\n",
            "```java\n",
            "public int binarySearch(int[] arr, int target) {\n",
            "    int left = 0, right = arr.length - 1;\n",
            "    while (left <= right) {\n",
            "        int mid = (left + right) / 2;\n",
            "        if (arr[mid] == target) {\n",
            "            return mid;\n",
            "        } else if (arr[mid] < target) {\n",
            "            left = mid + 1;\n",
            "        } else {\n",
            "            right = mid - 1;\n",
            "        }\n",
            "    }\n",
            "    return -1;\n",
            "}\n",
            "```\n",
            "\n",
            "16\n",
            "```cpp\n",
            "bool isEven(int x) {\n",
            "    return x % 2 == 0;\n",
            "}\n",
            "```\n",
            "```cpp\n",
            "bool isEven(int x) {\n",
            "    return x % 2 == 0;\n",
            "}\n",
            "```\n",
            "\n",
            "17\n",
            "```javascript\n",
            "function isEven(n) {\n",
            "    return n % 2 === 0;\n",
            "}\n",
            "```\n",
            "```javascript\n",
            "function isEven(n) {\n",
            "    return n % 2 === 0;\n",
            "}\n",
            "```\n",
            "\n",
            "18\n",
            "```c\n",
            "int process(int x) {\n",
            "    if (x < 0) {\n",
            "        return -1;\n",
            "    }\n",
            "    int result = x * x;\n",
            "    return result;\n",
            "}\n",
            "```\n",
            "1. Define a function named `process` that takes an integer input `x`.\n",
            "2. Check if `x` is less than 0:\n",
            "   - If true, return -1.\n",
            "3. If `x` is 0 or greater, calculate the square of `x` by multiplying `x` by itself.\n",
            "4. Return the squared result.\n",
            "\n",
            "Bleu-4 Score: 1.3412188165867924\n",
            "\n",
            "19\n",
            "```python\n",
            "def calculate_average(scores):\n",
            "    total = 0\n",
            "    for score in scores:\n",
            "        total += score\n",
            "    return total / len(scores)\n",
            "```\n",
            "```python\n",
            "def calculate_average(scores):\n",
            "    total = sum(scores)\n",
            "    return total / len(scores) if scores else 0\n",
            "```\n",
            "\n",
            "20\n",
            "```python\n",
            "# utils.py - Script to parse and summarize numeric CSV files\n",
            "import csv\n",
            "\n",
            "def read_csv(filepath):\n",
            "    with open(filepath, 'r') as f:\n",
            "        return list(csv.reader(f))\n",
            "\n",
            "def summarize_column(data, index):\n",
            "    if not data or len(data) < 2:\n",
            "        raise ValueError(\"Data must contain at least one header row and one data row.\")\n",
            "    values = [float(row[index]) for row in data[1:]]  # skip header\n",
            "    total = sum(values)\n",
            "    avg = total / len(values)\n",
            "    return total, avg\n",
            "\n",
            "def main():\n",
            "    filepath = 'data.csv'\n",
            "    data = read_csv(filepath)\n",
            "    total, avg = summarize_column(data, 1)\n",
            "    print(\"Total:\", total)\n",
            "    print(\"Average:\", avg)\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    main()\n",
            "```\n",
            "```python\n",
            "import csv\n",
            "\n",
            "def read_csv(filepath):\n",
            "    with open(filepath, 'r') as f:\n",
            "        return [row for row in csv.reader(f)]\n",
            "\n",
            "def summarize_column(data, index):\n",
            "    values = [float(row[index]) for row in data[1:] if row[index]]\n",
            "    if not values:\n",
            "        return 0, 0\n",
            "    total = sum(values)\n",
            "    avg = total / len(values)\n",
            "    return total, avg\n",
            "\n",
            "def main():\n",
            "    filepath = 'data.csv'\n",
            "    data = read_csv(filepath)\n",
            "    if len(data) > 1:\n",
            "        total, avg = summarize_column(data, 1)\n",
            "        print(\"Total:\", total)\n",
            "        print(\"Average:\", avg)\n",
            "    else:\n",
            "        print(\"No data to summarize.\")\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    main()\n",
            "```\n",
            "\n",
            "21\n",
            "```python\n",
            "# file_processor.py - Incomplete script for processing text files\n",
            "import string\n",
            "\n",
            "def load_file(filepath):\n",
            "    with open(filepath, 'r') as f:\n",
            "        return f.readlines()\n",
            "\n",
            "def clean_line(line):\n",
            "    # Remove punctuation and make lowercase\n",
            "    return line.translate(str.maketrans('', '', string.punctuation)).lower()\n",
            "\n",
            "def count_words(lines):\n",
            "    word_counts = {}\n",
            "    for line in lines:\n",
            "        clean = clean_line(line)\n",
            "        for word in clean.split():\n",
            "            word_counts[word] = word_counts.get(word, 0) + 1\n",
            "    return word_counts\n",
            "\n",
            "def main():\n",
            "    filepath = 'input.txt'\n",
            "    lines = load_file(filepath)\n",
            "    counts = count_words(lines)\n",
            "    for word, count in sorted(counts.items()):\n",
            "        print(f\"{word}: {count}\")\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    main()\n",
            "```\n",
            "```python\n",
            "# file_processor.py - Incomplete script for processing text files\n",
            "import string\n",
            "\n",
            "def load_file(filepath):\n",
            "    with open(filepath, 'r') as f:\n",
            "        return f.readlines()\n",
            "\n",
            "def clean_line(line):\n",
            "    return line.translate(str.maketrans('', '', string.punctuation)).lower()\n",
            "\n",
            "def count_words(lines):\n",
            "    word_counts = {}\n",
            "    for line in lines:\n",
            "        clean = clean_line(line)\n",
            "        for word in clean.split():\n",
            "            word_counts[word] = word_counts.get(word, 0) + 1\n",
            "    return word_counts\n",
            "\n",
            "def main():\n",
            "    filepath = 'input.txt'\n",
            "    lines = load_file(filepath)\n",
            "    counts = count_words(lines)\n",
            "    for word, count in sorted(counts.items()):\n",
            "        print(f\"{word}: {count}\")\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    main()\n",
            "```\n",
            "\n",
            "['Bleu-4 Score: 30.65076', 'Match: True', 'Bleu-4 Score: 2.74093', 'Match: False', 'Match: False', 'Match: False', 'Bleu-4 Score: 51.54409', 'Match: False', 'Match: False', 'Bleu-4 Score: 39.6663', 'Bleu-4 Score: 18.14559', 'Match: True', 'Match: False', 'Match: False', 'Match: True', 'Match: False', 'Match: True', 'Match: True', 'Bleu-4 Score: 1.34122', 'Match: False', 'Match: False', 'Match: False']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = New_Scores"
      ],
      "metadata": {
        "id": "489N2JHV3Aac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# with open('Codestral.json', 'w') as json_file:\n",
        "#     json.dump(predictions, json_file, indent=4)\n",
        "\n",
        "# with open('GPT.json', 'w') as json_file:\n",
        "#   json.dump(references, json_file, indent=4)\n",
        "\n",
        "# with open('Scores.json', 'w') as json_file:\n",
        "#   json.dump(scores, json_file, indent=4)"
      ],
      "metadata": {
        "id": "Smgthdr_61DI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predictions = []\n",
        "# references = []\n",
        "# scores = []"
      ],
      "metadata": {
        "id": "N0GVqf6MEDr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('Codestral.json', 'r') as json_file:\n",
        "#     predictions = json.load(json_file)\n",
        "\n",
        "# with open('GPT.json', 'r') as json_file:\n",
        "#   references = json.load(json_file)\n",
        "# with open('Scores.json', 'r') as json_file:\n",
        "#   scores = json.load(json_file)"
      ],
      "metadata": {
        "id": "yKbDqPA7pa4M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "collapsed": true,
        "outputId": "78238e75-21cb-411f-b2c9-56bdc0e3dab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Codestral.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ac1f770b30de>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Codestral.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPT.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mreferences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Codestral.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Format the data into\n",
        "\n",
        "data = []\n",
        "\n",
        "for i, prompt in enumerate(code_1):\n",
        "    for model_name in [\"Codestral-2501\", \"GPT-4o-mini\"]:\n",
        "      if model_name == \"Codestral-2501\":\n",
        "        output = predictions[i]\n",
        "      elif model_name == \"GPT-4o-mini\":\n",
        "        output= references[i] # Your function\n",
        "      score = scores[i]  # Your metric\n",
        "      analysis = \"Generic text\"  # Optional\n",
        "      data.append({\n",
        "            \"prompt_id\": i + 1,\n",
        "            \"parameters\": \"max_tokens=1024,\\n temperature = 0.3\",\n",
        "            \"prompt_text\": prompt,\n",
        "            \"model_name\": model_name,\n",
        "            \"output_text\": output,\n",
        "            \"score\": score,\n",
        "            # \"analysis\": analysis\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(data)\n"
      ],
      "metadata": {
        "id": "GVgdI0OG5jAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('zero_shot_final.csv', index=False)"
      ],
      "metadata": {
        "id": "0s4SUaM5llU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gOzbDqA5Zm5r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}